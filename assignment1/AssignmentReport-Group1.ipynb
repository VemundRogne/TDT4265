{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report Vemund Rogne and Kristian Brudeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "We can get the desired gradient $\\frac{\\partial C^n(w)}{\\partial w_i}$ by use of the chain rule: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C^n(w)}{\\partial w_i} &= \\frac{\\partial C^n(w)}{\\partial \\hat y} \\cdot \\frac{\\partial \\hat y}{\\partial f} \\cdot \\frac{\\partial f}{\\partial w_i} \\\\ \n",
    "\\frac{\\partial C^n(w)}{\\partial \\hat y} &= -(y^n \\cdot \\frac{1}{\\hat y^n} + (1-y^n)\\cdot(-1)\\cdot\\frac{1}{1-\\hat y^n}) \\\\ \n",
    "&=-\\bigg(\\frac{y^n}{\\hat y^n} - \\frac{1-y^n}{1-\\hat y^n} \\bigg) \\\\ \\\\\n",
    "\\frac{\\partial \\hat y}{\\partial f} &= 1 \\\\ \\\\\n",
    "\\frac{\\partial f}{\\partial w_i} &= x_i^nf(x^n)(1-f(x^n)) = x_i^n \\hat y^n (1-\\hat y^n) \\\\ \\\\\n",
    "\\frac{\\partial C^n(w)}{\\partial w_i} &= -x_i^n\\hat y^n(1-\\hat y^n)\\bigg(\\frac{y^n}{\\hat y^n}+\\frac{1-y^n}{1-\\hat y^n}\\bigg) \\\\\n",
    "&= -x_i^n(y^n(1-\\hat y^n) - (1-y^n)\\hat y^n) \\\\\n",
    "&= -x_i^n(y^n-y^n \\hat y^n-\\hat y^n+y^n\\hat y^n) \\\\\n",
    "&= -(y^n-\\hat y^n)x_i^n\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "The superscript \"n\" (for sample number) is implicit for all \"y\" and \"x\" variables to avoid to cluttered notation, e.g. $ \\hat y \\equiv \\hat y^n $ etc.\n",
    "\n",
    "First, we calculate the softmax derivative w.r.t. the network outputs:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{\\partial \\hat{y_i}}{\\partial z_k} = \\frac{\\partial \\hat{}}{\\partial z_k}(\\frac{e^{z_i}}{\\sum_{k'}^{K} e^{z_{k'}}}) \\\\\n",
    "    i = k: \\quad \\frac{\\partial \\hat{y_k}}{\\partial z_k} &=  \\frac\n",
    "    {(\\frac{\\partial \\hat{}}{\\partial z_k} e^{z_{k'}}) \\sum_{k'}^{K} e^{z_{k'}} - (\\frac{\\partial \\hat{}}{\\partial z_k}\\sum_{k'}^{K} e^{z_{k'}}) e^{z_{k}}}\n",
    "    {(\\sum_{k'}^{K} e^{z_{k'}})^2} \\\\\n",
    "\t&=  \\frac\n",
    "    {e^{z_{k}}(\\sum_{k'}^{K} e^{z_{k'}}) - e^{z_{k}} e^{z_{k}}  }\n",
    "    {(\\sum_{k'}^{K} e^{z_{k'}})^2} \\\\\n",
    "\t&= \\frac{e^{z_{k}}}{\\sum_{k'}^{K} e^{z_{k'}}} \\cdot \\frac{\\sum_{k'}^{K} e^{z_{k'}} - e^{z_{k}}}{\\sum_{k'}^{K} e^{z_{k'}}} \\\\\n",
    "\t&= \\hat{y_k}(1 - \\hat{y_k}) \\\\\n",
    "    i \\neq k: \\quad \\frac{\\partial \\hat{y_i}}{\\partial z_k} &= -e^{z_i} \\frac{\\partial}{\\partial z_k}(\\frac{1}{\\sum_{k'}^{K} e^{z_{k'}}}) \\\\\n",
    "\t&= -e^{z_i} \\frac{e^{z_k}}{(\\sum_{k'}^{K} e^{z_{k'}})^2} \\\\\n",
    "\t&= - \\hat{y_i^n}\\hat{y_k^n}\n",
    "\\end{align*}\n",
    "$$\n",
    "Using the chain rule, we have that:\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\t\\frac{\\partial C(w)}{w_{kj}} &= \\sum_i \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_{kj}} = \\frac{\\partial C}{\\partial z_k}\\frac{\\partial z_k}{\\partial w_{kj}} + \\sum_{i \\neq k} \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w_{kj}} \\\\\n",
    "\t&= \\frac{\\partial C}{\\partial z_k}\\frac{\\partial z_k}{\\partial w_{kj}}\n",
    "\\end{align*}\n",
    "$$\n",
    "Where the last equality comes from the fact that $$\\frac{\\partial z_i}{\\partial w_{kj}} = 0 \\quad \\mathrm{for} \\quad i \\neq k $$\n",
    "\n",
    "Therefore\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\partial C(w)}{\\partial z_k} &= \\frac{\\partial}{\\partial z_k} (- \\sum_i^K y_i ln(\\hat{y}_i)) \\\\\n",
    "\t&= - \\sum_i^K y_i^n \\cdot \\frac{\\partial ln(\\hat{y}_i)}{\\partial \\hat{y_i}} \\cdot \\frac{\\partial \\hat{y_i}}{\\partial z_k} \\\\\n",
    "\t&= - \\sum_i^K y_i^n \\cdot \\frac{1}{\\hat{y_i}} \\cdot \\frac{\\partial \\hat{y_i}}{\\partial z_k} \\\\\n",
    "\t&= - \\bigg[\\sum_{i \\neq k}^K y_i^n \\cdot \\frac{1}{\\hat{y_i}} \\cdot (-\\hat{y_i} \\hat{y_k})\\bigg] - y_k \\frac{1}{\\hat{y_k}} \\hat{y_k} (1 - \\hat{y_k}) \\\\\n",
    "\t&= - \\bigg[\\sum_{i \\neq k}^K y_i^n  \\cdot (- \\hat{y_k})\\bigg] - y_k (1 - \\hat{y_k}) \\\\\n",
    "\t&= \\bigg[(\\sum_{i}^K y_i^n \\hat{y_k}) - y_k\\hat{y_k}\\bigg] - y_k + y_k\\hat{y_k} \\\\\n",
    "\t&= \\bigg[\\hat{y_k}(\\sum_{i}^K y_i^n ) \\bigg] - y_k \\\\\n",
    "\t&= - ( y_k - \\hat{y_k} )\n",
    "\\end{align*}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial z_k}{\\partial w_{kj}} = \\frac{\\partial}{\\partial w_{kj}} \\sum_i^I w_{ki}x_j = x_j\n",
    "$$\n",
    "\n",
    "Thus, the gradient of the loss function w.r.t. the weights is:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\partial C(w)}{\\partial w_{kj}} &= \\frac{\\partial C(w)}{\\partial z_k} \\frac{\\partial z_k}{\\partial w_{kj}} \\\\\n",
    "\t&= -(y_k - \\hat{y_k})x_j\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2a)\n",
    "The functionality was implemented in `task2a.py`.\n",
    "\n",
    "## Task 2b)\n",
    "Logistic regression with mini-batch descent was for a single-layer neural network was implemented in `task2.py`.\n",
    "\n",
    "![](figures/task2b_binary_train_loss.png)\n",
    "\n",
    "The figure shows the loss of the binary classifier without shuffled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "The function `calculate accuracy` was implemented in `task2.py`.\n",
    "\n",
    "![](figures/task2b_binary_train_accuracy.png)\n",
    "\n",
    "The figure shows the accuracy of the binary classifier without shuffled data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping was implemented in the training loop in `trainer.py`.\n",
    "\n",
    "Early stopping kicks in after 16 epochs. As we see in the graph, this is about halfway through the training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "Dataset shuffling was implemented in `batch_loader` in `utils.py`.\n",
    "\n",
    "![](figures/task2e_train_accuracy_shuffle_difference.png)\n",
    "\n",
    "Shuffling the data may help the training avoid the ordering of the training data against consistently affecting the performance of our classifier. For instance, if one part of the training data is very similar (or very different) from the validation data, we may expect the algorithm to perform better (or worse) after just optimizing the classifier to this data, in a sense overfitting to a part of the data. It seems that shuffling the data at every epoch reduces the variance of both training and validation accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3a)\n",
    "\n",
    "The functionality was implemented in `task3a.py`.\n",
    "\n",
    "## Task 3b)\n",
    "Softmax regression was implemented in `task3.py`. The multiclass classifier was trained on the data. The data is shuffled at each epoch. \n",
    "\n",
    "![](figures/task3b_softmax_train_loss.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "A function for calculating the accuracy on multiple classes was implemented. The plot shows the training and validation accuracy over training.\n",
    "\n",
    "![](figures/task3b_softmax_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "We see slight signs of overfitting, as we see that after about 2000 to 3000 training steps the accuracy on the validation data really starts to flatten out while the accuracy on the training data continues increasing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\t\\frac{\\partial J}{\\partial w} &= \\frac{\\partial C(w)}{\\partial w} + \\lambda \\frac{\\partial R(w)}{\\partial w} \\\\\n",
    "\t\\frac{\\partial R}{\\partial w} &= \\frac{\\partial}{\\partial w}\\frac{1}{2} ||w||^2 = \\frac{\\partial}{\\partial w} w^\\intercal \n",
    "\tw = w^\\intercal \\\\\n",
    "\t\\frac{\\partial J}{\\partial w} &= -(y_k - \\hat{y_k})x + w\n",
    " \\end{align*}\n",
    "$$\n",
    "\n",
    "We point out that this is a slight abuse of notation, because usually $\\frac{\\partial J}{\\partial w}$ would be a row vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "L2 Regularization was implemented, the resulting weights after training can be seen plotted below.\n",
    "\n",
    "![](figures/task4b_softmax_weight.png)\n",
    "\n",
    "The weights are less noisy as there is now a cost to larger weights. An explantion for why this may lead to less noisy weights is that the weight on a pixel that may have little information value does not contribute that much to decreasing the loss on the dataset at large is attenuated - giving less noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "We see that the validation accuracy degrades with regularization. In this case, there is therefore a trade-off between denoising the network weights through regularization and validation accuracy. \n",
    "\n",
    "One reason may be that in this case, the \"noisy\" weights may be better for distinguishing numbers that have spatial perturbations (e.g. they may be shifted horizontally or vertically, or have different curves) even though the regularized weights look nicer to the human eye. Some classifiers like convolutional neural networks with max pooling layers may circumvent this, but our network does not.\n",
    "\n",
    "![](figures/task4c_l2_reg_accuracy.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "\n",
    "It seems that because any amount of regularization imposes a cost on weights that may have contributed to the predictive power of the classifier. It may be that the early stopping and simple classifier parameterization makes the regularization less necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "\n",
    "![](figures/task4d_l2_reg_norms.png)\n",
    "\n",
    "We see that the norm of the network weights decreases when the regularization parameter lambda increases. This is expected, as the cost of larger network weights increases with increasing lambda."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
